[
    {
        "desc": "Specifies the maximum number of threads to use for transferring data in and out of the DataNode.",
        "display_name": "Maximum Number of Transfer Threads",
        "name": "dfs_datanode_max_xcievers",
        "value": "4096"
    },
    {
        "desc": "Whether or not periodic stacks collection is enabled.",
        "display_name": "Stacks Collection Enabled",
        "name": "stacks_collection_enabled",
        "value": "false"
    },
    {
        "desc": "Whether to suppress the results of the Transceiver Usage heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Transceiver Usage",
        "name": "role_health_suppression_data_node_transceivers_usage",
        "value": "false"
    },
    {
        "desc": "In some workloads, the data read from HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is delivered to the client. This may improve performance for some workloads by freeing buffer cache spare usage for more cacheable data. This behavior will always be disabled for workloads that read only short sections of a block (e.g HBase random-IO workloads). This property is supported in CDH3u3 or later deployments.",
        "display_name": "Enable purging cache after reads",
        "name": "dfs_datanode_drop_cache_behind_reads",
        "value": "false"
    },
    {
        "desc": "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
        "display_name": "Stacks Collection Method",
        "name": "stacks_collection_method",
        "value": "jstack"
    },
    {
        "desc": "Whether to suppress the results of the Audit Pipeline Test heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Audit Pipeline Test",
        "name": "role_health_suppression_data_node_audit_health",
        "value": "false"
    },
    {
        "desc": "Whether DataNodes should use DataNode hostnames when connecting to DataNodes for data transfer. This property is supported in CDH3u4 or later deployments.",
        "display_name": "Use DataNode Hostname",
        "name": "dfs_datanode_use_datanode_hostname",
        "value": "false"
    },
    {
        "desc": "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags, PermGen, or extra debugging flags would be passed here.",
        "display_name": "Java Configuration Options for DataNode",
        "name": "datanode_java_opts",
        "value": "-XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled"
    },
    {
        "desc": "The period to review when computing unexpected exits.",
        "display_name": "Unexpected Exits Monitoring Period",
        "name": "unexpected_exits_window",
        "value": "5"
    },
    {
        "desc": "Enables the health test that the DataNode's process state is consistent with the role configuration",
        "display_name": "DataNode Process Health Test",
        "name": "datanode_scm_health_enabled",
        "value": "true"
    },
    {
        "desc": "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
        "display_name": "DataNode Logging Advanced Configuration Snippet (Safety Valve)",
        "name": "log4j_safety_valve",
        "value": null
    },
    {
        "desc": "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
        "display_name": "Java Heap Size of DataNode in Bytes",
        "name": "datanode_java_heapsize",
        "value": "1073741824"
    },
    {
        "desc": "Directory where DataNode will place its log files.",
        "display_name": "DataNode Log Directory",
        "name": "datanode_log_dir",
        "value": "/var/log/hadoop-hdfs"
    },
    {
        "desc": "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
        "display_name": "Enable Health Alerts for this Role",
        "name": "enable_alerts",
        "value": "false"
    },
    {
        "desc": "Whether to suppress the results of the Web Server Status heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Web Server Status",
        "name": "role_health_suppression_data_node_web_metric_collection",
        "value": "false"
    },
    {
        "desc": "While reading block files, the DataNode can use the posix_fadvise system call to explicitly page data into the operating system buffer cache ahead of the current reader's position. This can improve performance especially when disks are highly contended. This configuration specifies the number of bytes ahead of the current read position which the DataNode will attempt to read ahead. A value of 0 disables this feature. This property is supported in CDH3u3 or later deployments.",
        "display_name": "Number of read ahead bytes",
        "name": "dfs_datanode_readahead_bytes",
        "value": "4194304"
    },
    {
        "desc": "Only used when the DataNode Volume Choosing Policy is set to Available Space. Controls how much DataNode volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced. If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments will be done on a pure round robin basis.",
        "display_name": "Available Space Policy Balanced Threshold",
        "name": "dfs_datanode_available_space_balanced_threshold",
        "value": "10737418240"
    },
    {
        "desc": "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
        "display_name": "Dump Heap When Out of Memory",
        "name": "oom_heap_dump_enabled",
        "value": "true"
    },
    {
        "desc": "The health test thresholds on the duration of the metrics request to the web server.",
        "display_name": "Web Metric Collection Duration",
        "name": "datanode_web_metric_collection_thresholds",
        "value": "{\"critical\":\"never\",\"warning\":\"10000.0\"}"
    },
    {
        "desc": "The health test thresholds for monitoring of free space on the filesystem that contains this role's DataNode Data Directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a DataNode Data Directory Free Space Monitoring Absolute Thresholds setting is configured.",
        "display_name": "DataNode Data Directory Free Space Monitoring Percentage Thresholds",
        "name": "datanode_data_directories_free_space_percentage_thresholds",
        "value": "{\"critical\":\"never\",\"warning\":\"never\"}"
    },
    {
        "desc": "Port for the various DataNode Protocols. Combined with the DataNode's hostname to build its IPC port address.",
        "display_name": "DataNode Protocol Port",
        "name": "dfs_datanode_ipc_port",
        "value": "50020"
    },
    {
        "desc": "The period to review when computing the moving average of extra time the pause monitor spent paused.",
        "display_name": "Pause Duration Monitoring Period",
        "name": "datanode_pause_duration_window",
        "value": "5"
    },
    {
        "desc": "In some workloads, the data written to HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is written to disk. This may improve performance for some workloads by freeing buffer cache spare usage for more cacheable data. This property is supported in CDH3u3 or later deployments.",
        "display_name": "Enable purging cache after writes",
        "name": "dfs_datanode_drop_cache_behind_writes",
        "value": "false"
    },
    {
        "desc": "The health test thresholds for monitoring of free space on the filesystem that contains this role's DataNode Data Directory.",
        "display_name": "DataNode Data Directory Free Space Monitoring Absolute Thresholds",
        "name": "datanode_data_directories_free_space_absolute_thresholds",
        "value": "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}"
    },
    {
        "desc": "Permissions for the directories on the local file system where the DataNode stores its blocks. The permissions must be octal. 755 and 700 are typical values.",
        "display_name": "DataNode Data Directory Permissions",
        "name": "dfs_datanode_data_dir_perm",
        "value": "700"
    },
    {
        "desc": "Maximum amount of bandwidth that each DataNode can use for balancing. Specified in bytes per second.",
        "display_name": "DataNode Balancing Bandwidth",
        "name": "dfs_balance_bandwidthPerSec",
        "value": "10485760"
    },
    {
        "desc": "The health test thresholds of transceivers usage in a DataNode. Specified as a percentage of the total configured number of transceivers.",
        "display_name": "DataNode Transceivers Usage Thresholds",
        "name": "datanode_transceivers_usage_thresholds",
        "value": "{\"critical\":\"95.0\",\"warning\":\"75.0\"}"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DataNode Data Directory parameter.",
        "display_name": "Suppress Parameter Validation: DataNode Data Directory",
        "name": "role_config_suppression_dfs_data_dir_list",
        "value": "false"
    },
    {
        "desc": "Whether to suppress the results of the File Descriptors heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: File Descriptors",
        "name": "role_health_suppression_data_node_file_descriptor",
        "value": "false"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Stacks Collection Directory parameter.",
        "display_name": "Suppress Parameter Validation: Stacks Collection Directory",
        "name": "role_config_suppression_stacks_collection_directory",
        "value": "false"
    },
    {
        "desc": "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
        "display_name": "Log Directory Free Space Monitoring Percentage Thresholds",
        "name": "log_directory_free_space_percentage_thresholds",
        "value": "{\"critical\":\"never\",\"warning\":\"never\"}"
    },
    {
        "desc": "<p>This file contains the rules that govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message. If a log message matches multiple rules, the first matching rule is used. </p><p>Each rule has some or all of the following fields:</p><ul><li><code>alert</code> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><code>rate</code> <b>(mandatory)</b> - the maximum number of log messages matching this rule that can be sent as events every minute. If more than <code>rate</code> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><code>periodminutes</code>  - the number of minutes during which the publisher will only publish <code>rate</code> events or fewer. If not specified, the default is <b>one minute</b></li><li><code>threshold</code> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><code>content</code> - match only those messages for which contents match this regular expression.</li><li><code>exceptiontype</code> - match only those messages that are part of an exception message. The exception type must match this regular expression.</li></ul><p>Example:</p><ul><li><pre>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</pre>This rule sends events to Cloudera Manager for every <code>StringIndexOutOfBoundsException</code>, up to a maximum of 10 every minute.</li><li><pre>{\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"exceptiontype\": \".*\"}, {\"alert\": true, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"ERROR\"}</pre>In this example, an event generated may not be promoted to alert if an exception is in the ERROR log message, because the first rule with alert = false will match.</li></ul>",
        "display_name": "Rules to Extract Events from Log Files",
        "name": "log_event_whitelist",
        "value": "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Instead, use .*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Use .* instead\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.IOException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketClosedException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.EOFException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.nio.channels.CancelledKeyException\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 5, \"content\":\"Datanode registration failed\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Got a command from standby NN - ignoring command:.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Unknown job [^ ]+ being deleted.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Error executing shell command .+ No such process.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\".*attempt to override final parameter.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"[^ ]+ is a deprecated filesystem name. Use.*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"}\n  ]\n}\n"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the DataNode Failed Volumes Tolerated Validator configuration validator.",
        "display_name": "Suppress Configuration Validator: DataNode Failed Volumes Tolerated Validator",
        "name": "role_config_suppression_datanode_failed_volumes_validator",
        "value": "false"
    },
    {
        "desc": "Whether to suppress the results of the Data Directory Status heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Data Directory Status",
        "name": "role_health_suppression_data_node_volume_failures",
        "value": "false"
    },
    {
        "desc": "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
        "display_name": "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
        "name": "heap_dump_directory_free_space_absolute_thresholds",
        "value": "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}"
    },
    {
        "desc": "Whether to suppress the results of the DataNode Data Directory Free Space heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: DataNode Data Directory Free Space",
        "name": "role_health_suppression_datanode_data_directories_free_space",
        "value": "false"
    },
    {
        "desc": "When computing the overall DataNode health, consider the host's health.",
        "display_name": "DataNode Host Health Test",
        "name": "datanode_host_health_enabled",
        "value": "true"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Java Configuration Options for DataNode parameter.",
        "display_name": "Suppress Parameter Validation: Java Configuration Options for DataNode",
        "name": "role_config_suppression_datanode_java_opts",
        "value": "false"
    },
    {
        "desc": "The maximum size, in megabytes, per log file for DataNode logs.  Typically used by log4j or logback.",
        "display_name": "DataNode Max Log Size",
        "name": "max_log_size",
        "value": "200"
    },
    {
        "desc": "<p>The configured triggers for this role. This is a JSON-formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has the following fields:</p><ul><li><code>triggerName</code> <b>(mandatory)</b> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <b>(mandatory)</b> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <b>(optional)</b> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <b> (optional)</b> - By default set to 'true'. If set to 'false', the trigger is not evaluated.</li><li><code>expressionEditorConfig</code> <b> (optional)</b> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here can lead to inconsistencies.</li></ul><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change and, as a result, backward compatibility is not guaranteed between releases.</p>",
        "display_name": "Role Triggers",
        "name": "role_triggers",
        "value": "[]"
    },
    {
        "desc": "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
        "display_name": "File Descriptor Monitoring Thresholds",
        "name": "datanode_fd_thresholds",
        "value": "{\"critical\":\"70.0\",\"warning\":\"50.0\"}"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DateNode Plugins parameter.",
        "display_name": "Suppress Parameter Validation: DateNode Plugins",
        "name": "role_config_suppression_dfs_datanode_plugins_list",
        "value": "false"
    },
    {
        "desc": "Whether to suppress the results of the NameNode Connectivity heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: NameNode Connectivity",
        "name": "role_health_suppression_data_node_ha_connectivity",
        "value": "false"
    },
    {
        "desc": "Advanced Configuration Snippet (Safety Valve) for Hadoop Metrics2. Properties will be inserted into <strong>hadoop-metrics2.properties</strong>.",
        "display_name": "Hadoop Metrics2 Advanced Configuration Snippet (Safety Valve)",
        "name": "hadoop_metrics2_safety_valve",
        "value": null
    },
    {
        "desc": "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
        "display_name": "Stacks Collection Data Retention",
        "name": "stacks_collection_data_retention",
        "value": "104857600"
    },
    {
        "desc": "Port for the DataNode HTTP web UI. Combined with the DataNode's hostname to build its HTTP address.",
        "display_name": "DataNode HTTP Web UI Port",
        "name": "dfs_datanode_http_port",
        "value": "50075"
    },
    {
        "desc": "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
        "display_name": "Unexpected Exits Thresholds",
        "name": "unexpected_exits_thresholds",
        "value": "{\"critical\":\"any\",\"warning\":\"never\"}"
    },
    {
        "desc": "The health test thresholds of free space in a DataNode. Specified as a percentage of the capacity on the DataNode.",
        "display_name": "DataNode Free Space Monitoring Thresholds",
        "name": "datanode_free_space_thresholds",
        "value": "{\"critical\":\"10.0\",\"warning\":\"20.0\"}"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Hadoop Metrics2 Advanced Configuration Snippet (Safety Valve) parameter.",
        "display_name": "Suppress Parameter Validation: Hadoop Metrics2 Advanced Configuration Snippet (Safety Valve)",
        "name": "role_config_suppression_hadoop_metrics2_safety_valve",
        "value": "false"
    },
    {
        "desc": "Port for DataNode's XCeiver Protocol. Combined with the DataNode's hostname to build its address.",
        "display_name": "DataNode Transceiver Port",
        "name": "dfs_datanode_port",
        "value": "50010"
    },
    {
        "desc": "The health test thresholds of the number of blocks on a DataNode",
        "display_name": "DataNode Block Count Thresholds",
        "name": "datanode_block_count_thresholds",
        "value": "{\"critical\":\"never\",\"warning\":\"500000.0\"}"
    },
    {
        "desc": "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
        "display_name": "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
        "name": "heap_dump_directory_free_space_percentage_thresholds",
        "value": "{\"critical\":\"never\",\"warning\":\"never\"}"
    },
    {
        "desc": "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
        "display_name": "Cgroup CPU Shares",
        "name": "rm_cpu_shares",
        "value": "1024"
    },
    {
        "desc": "The health test thresholds of failed volumes in a DataNode.",
        "display_name": "DataNode Volume Failures Thresholds",
        "name": "datanode_volume_failures_thresholds",
        "value": "{\"critical\":\"any\",\"warning\":\"never\"}"
    },
    {
        "desc": "Whether to suppress the results of the Heap Dump Directory Free Space heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Heap Dump Directory Free Space",
        "name": "role_health_suppression_data_node_heap_dump_directory_free_space",
        "value": "false"
    },
    {
        "desc": "The minimum log level for DataNode logs",
        "display_name": "DataNode Logging Threshold",
        "name": "log_threshold",
        "value": "INFO"
    },
    {
        "desc": "Whether to suppress the results of the Process Status heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Process Status",
        "name": "role_health_suppression_data_node_scm_health",
        "value": "false"
    },
    {
        "desc": "DataNode Policy for picking which volume should get a new block. The Available Space policy is only available starting with CDH 4.3.",
        "display_name": "DataNode Volume Choosing Policy",
        "name": "dfs_datanode_volume_choosing_policy",
        "value": "org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DataNode Data Directory Permissions parameter.",
        "display_name": "Suppress Parameter Validation: DataNode Data Directory Permissions",
        "name": "role_config_suppression_dfs_datanode_data_dir_perm",
        "value": "false"
    },
    {
        "desc": "The health test thresholds for the weighted average extra time the pause monitor spent paused. Specified as a percentage of elapsed wall clock time.",
        "display_name": "Pause Duration Thresholds",
        "name": "datanode_pause_duration_thresholds",
        "value": "{\"critical\":\"60.0\",\"warning\":\"30.0\"}"
    },
    {
        "desc": "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
        "display_name": "Automatically Restart Process",
        "name": "process_auto_restart",
        "value": "true"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DataNode Logging Advanced Configuration Snippet (Safety Valve) parameter.",
        "display_name": "Suppress Parameter Validation: DataNode Logging Advanced Configuration Snippet (Safety Valve)",
        "name": "role_config_suppression_log4j_safety_valve",
        "value": "false"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DataNode Environment Advanced Configuration Snippet (Safety Valve) parameter.",
        "display_name": "Suppress Parameter Validation: DataNode Environment Advanced Configuration Snippet (Safety Valve)",
        "name": "role_config_suppression_datanode_role_env_safety_valve",
        "value": "false"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Rules to Extract Events from Log Files parameter.",
        "display_name": "Suppress Parameter Validation: Rules to Extract Events from Log Files",
        "name": "role_config_suppression_log_event_whitelist",
        "value": "false"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the CDH Version Validator configuration validator.",
        "display_name": "Suppress Configuration Validator: CDH Version Validator",
        "name": "role_config_suppression_cdh_version_validator",
        "value": "false"
    },
    {
        "desc": "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
        "display_name": "Cgroup Memory Hard Limit",
        "name": "rm_memory_hard_limit",
        "value": "-1"
    },
    {
        "desc": "Whether to suppress the results of the Host Health heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Host Health",
        "name": "role_health_suppression_data_node_host_health",
        "value": "false"
    },
    {
        "desc": "The health test thresholds on the swap memory usage of the process.",
        "display_name": "Process Swap Memory Thresholds",
        "name": "process_swap_memory_thresholds",
        "value": "{\"critical\":\"never\",\"warning\":\"any\"}"
    },
    {
        "desc": "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
        "display_name": "Log Directory Free Space Monitoring Absolute Thresholds",
        "name": "log_directory_free_space_absolute_thresholds",
        "value": "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}"
    },
    {
        "desc": "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
        "display_name": "Maximum Process File Descriptors",
        "name": "rlimit_fds",
        "value": null
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the DataNode Reserved Space Validator configuration validator.",
        "display_name": "Suppress Configuration Validator: DataNode Reserved Space Validator",
        "name": "role_config_suppression_datanode_reserved_space_validator",
        "value": "false"
    },
    {
        "desc": "For advanced use only. A string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
        "display_name": "DataNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
        "name": "datanode_config_safety_valve",
        "value": null
    },
    {
        "desc": "The maximum amount of memory a DataNode may use to cache data blocks in memory.  Setting it to zero will disable caching.",
        "display_name": "Maximum Memory Used for Caching",
        "name": "dfs_datanode_max_locked_memory",
        "value": "4294967296"
    },
    {
        "desc": "Enables the health test that verifies the DataNode is connected to the NameNode",
        "display_name": "DataNode Connectivity Health Test",
        "name": "datanode_connectivity_health_enabled",
        "value": "true"
    },
    {
        "desc": "Minimum number of running threads for the HDFS Thrift server running on each DataNode",
        "display_name": "HDFS Thrift Server Min Threadcount",
        "name": "dfs_thrift_threads_min",
        "value": "10"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DataNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml parameter.",
        "display_name": "Suppress Parameter Validation: DataNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
        "name": "role_config_suppression_datanode_config_safety_valve",
        "value": "false"
    },
    {
        "desc": "The base port where the secure DataNode web UI listens.  Combined with the DataNode's hostname to build its secure web UI address.",
        "display_name": "Secure DataNode Web UI Port (TLS/SSL)",
        "name": "dfs_datanode_https_port",
        "value": "50475"
    },
    {
        "desc": "The amount of time to wait for the DataNode to fully start up and connect to the NameNode before enforcing the connectivity check.",
        "display_name": "DataNode Connectivity Tolerance at Startup",
        "name": "datanode_connectivity_tolerance",
        "value": "180"
    },
    {
        "desc": "Maximum number of running threads for the HDFS Thrift server running on each DataNode",
        "display_name": "HDFS Thrift Server Max Threadcount",
        "name": "dfs_thrift_threads_max",
        "value": "20"
    },
    {
        "desc": "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
        "display_name": "Heap Dump Directory",
        "name": "oom_heap_dump_dir",
        "value": "/tmp"
    },
    {
        "desc": "The frequency with which stacks are collected.",
        "display_name": "Stacks Collection Frequency",
        "name": "stacks_collection_frequency",
        "value": "5.0"
    },
    {
        "desc": "Whether to suppress the results of the Free Space heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Free Space",
        "name": "role_health_suppression_data_node_free_space_remaining",
        "value": "false"
    },
    {
        "desc": "If this configuration is enabled, the DataNode will instruct the operating system to enqueue all written data to the disk immediately after it is written. This differs from the usual OS policy which may wait for up to 30 seconds before triggering writeback. This may improve performance for some workloads by smoothing the IO profile for data written to disk. This property is supported in CDH3u3 or later deployments.",
        "display_name": "Enable immediate enqueuing of data to disk after writes",
        "name": "dfs_datanode_sync_behind_writes",
        "value": "false"
    },
    {
        "desc": "Whether to suppress the results of the Swap Memory Usage heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Swap Memory Usage",
        "name": "role_health_suppression_data_node_swap_memory_usage",
        "value": "false"
    },
    {
        "desc": "Comma-separated list of DataNode plug-ins to be activated. If one plug-in cannot be loaded, all the plug-ins are ignored.",
        "display_name": "DateNode Plugins",
        "name": "dfs_datanode_plugins_list",
        "value": ""
    },
    {
        "desc": "Comma-delimited list of directories on the local file system where the DataNode stores HDFS block data. Typical values are /data/N/dfs/dn for N = 1, 2, 3.... In CDH 5.7 and higher, these directories can be optionally tagged with their storage types, for example, [SSD]/data/1/dns/dn. HDFS supports the following storage types: [DISK], [SSD], [ARCHIVE], [RAM_DISK]. The default storage type of a directory will be [DISK] if it does not have a storage type tagged explicitly. These directories should be mounted using the noatime option, and the disks should be configured using JBOD. RAID is not recommended. <strong>Warning: Be very careful when modifying this property. Removing or changing entries can result in data loss.</strong> To hot swap drives in CDH 5.4 and higher, override the value of this property for the specific DataNode role instance that has the drive to be hot-swapped; do not modify the property value in the role group. See <a class=\"bold\" href=\"http://tiny.cloudera.com/hot-swap\" target=\"_blank\">Configuring Hot Swap for DataNodes<i class=\"externalLink\"></i></a> for more information.",
        "display_name": "DataNode Data Directory",
        "name": "dfs_data_dir_list",
        "value": null
    },
    {
        "desc": "Whether to suppress the results of the Pause Duration heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Pause Duration",
        "name": "role_health_suppression_data_node_pause_duration",
        "value": "false"
    },
    {
        "desc": "The number of volumes that are allowed to fail before a DataNode stops offering service. By default, any volume failure will cause a DataNode to shutdown.",
        "display_name": "DataNode Failed Volumes Tolerated",
        "name": "dfs_datanode_failed_volumes_tolerated",
        "value": "0"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Heap Dump Directory parameter.",
        "display_name": "Suppress Parameter Validation: Heap Dump Directory",
        "name": "role_config_suppression_oom_heap_dump_dir",
        "value": "false"
    },
    {
        "desc": "If enabled, the DataNode binds to the wildcard address (\"0.0.0.0\") on all of its ports.",
        "display_name": "Bind DataNode to Wildcard Address",
        "name": "dfs_datanode_bind_wildcard",
        "value": "false"
    },
    {
        "desc": "The number of server threads for the DataNode.",
        "display_name": "Handler Count",
        "name": "dfs_datanode_handler_count",
        "value": "3"
    },
    {
        "desc": "The maximum number of rolled log files to keep for DataNode logs.  Typically used by log4j or logback.",
        "display_name": "DataNode Maximum Log File Backups",
        "name": "max_log_backup_index",
        "value": "10"
    },
    {
        "desc": "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
        "display_name": "DataNode Environment Advanced Configuration Snippet (Safety Valve)",
        "name": "DATANODE_role_env_safety_valve",
        "value": null
    },
    {
        "desc": "Timeout in seconds for the HDFS Thrift server running on each DataNode",
        "display_name": "HDFS Thrift Server Timeout",
        "name": "dfs_thrift_timeout",
        "value": "60"
    },
    {
        "desc": "Whether to suppress the results of the Log Directory Free Space heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Log Directory Free Space",
        "name": "role_health_suppression_data_node_log_directory_free_space",
        "value": "false"
    },
    {
        "desc": "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
        "display_name": "Cgroup Memory Soft Limit",
        "name": "rm_memory_soft_limit",
        "value": "-1"
    },
    {
        "desc": "Whether to suppress the results of the Block Count heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Block Count",
        "name": "role_health_suppression_data_node_block_count",
        "value": "false"
    },
    {
        "desc": "Cloudera Manager agent monitors each service and each of its role by publishing metrics to the Cloudera Manager Service Monitor. Setting it to false will stop Cloudera Manager agent from publishing any metric for corresponding service/roles. This is usually helpful for services that generate large amount of metrics which Service Monitor is not able to process.",
        "display_name": "Enable Metric Collection",
        "name": "process_should_monitor",
        "value": "true"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the DataNode Log Directory parameter.",
        "display_name": "Suppress Parameter Validation: DataNode Log Directory",
        "name": "role_config_suppression_datanode_log_dir",
        "value": "false"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Role Triggers parameter.",
        "display_name": "Suppress Parameter Validation: Role Triggers",
        "name": "role_config_suppression_role_triggers",
        "value": "false"
    },
    {
        "desc": "Whether to suppress configuration warnings produced by the built-in parameter validation for the Java Heap Size of DataNode in Bytes parameter.",
        "display_name": "Suppress Parameter Validation: Java Heap Size of DataNode in Bytes",
        "name": "role_config_suppression_datanode_java_heapsize",
        "value": "false"
    },
    {
        "desc": "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
        "display_name": "Kill When Out of Memory",
        "name": "oom_sigkill_enabled",
        "value": "true"
    },
    {
        "desc": "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
        "display_name": "Enable Configuration Change Alerts",
        "name": "enable_config_alerts",
        "value": "false"
    },
    {
        "desc": "Reserved space in bytes per volume for non Distributed File System (DFS) use.",
        "display_name": "Reserved Space for Non DFS Use",
        "name": "dfs_datanode_du_reserved",
        "value": "10737418240"
    },
    {
        "desc": "The directory in which stacks logs are placed. If not set, stacks are logged into a <code>stacks</code> subdirectory of the role's log directory.",
        "display_name": "Stacks Collection Directory",
        "name": "stacks_collection_directory",
        "value": null
    },
    {
        "desc": "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
        "display_name": "Cgroup I/O Weight",
        "name": "rm_io_weight",
        "value": "500"
    },
    {
        "desc": "Only used when the DataNode Volume Choosing Policy is set to Available Space. Controls what percentage of new block allocations will be sent to volumes with more available disk space than others. This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer that volumes with less available disk space receive more block allocations.",
        "display_name": "Available Space Policy Balanced Preference",
        "name": "dfs_datanode_available_space_balanced_preference",
        "value": "0.75"
    },
    {
        "desc": "Whether to suppress the results of the Unexpected Exits heath test. The results of suppressed health tests are ignored when computing the overall health of the associated host, role or service, so suppressed health tests will not generate alerts.",
        "display_name": "Suppress Health Test: Unexpected Exits",
        "name": "role_health_suppression_data_node_unexpected_exits",
        "value": "false"
    },
    {
        "desc": "Enables the health test that the Cloudera Manager Agent can successfully contact and gather metrics from the web server.",
        "display_name": "Web Metric Collection",
        "name": "datanode_web_metric_collection_enabled",
        "value": "true"
    }
]